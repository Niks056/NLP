{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"NLP_Basics.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"TBPRD2wbrO1E","colab_type":"text"},"source":["# Remove Punctuation "]},{"cell_type":"code","metadata":{"id":"x_ecoankrO1G","colab_type":"code","colab":{}},"source":["import string\n","import pandas as pd\n","import numpy as np\n","import sklearn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dRqo5-jvrO1P","colab_type":"code","colab":{}},"source":["letters - k , p , s\n","words - sun, light\n","Sentences - today is a bright day \n","document - collection of sentences \n","corpus/dataset - collection of documents \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mYpfn-zhrO1S","colab_type":"code","colab":{}},"source":["sample_msg='This is a NLP notebook !. NLP is very useful .Notice: It takes little time to udnerstand NLP techniques'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rnyfDC9xrO1W","colab_type":"code","colab":{},"outputId":"a05f34d2-9a9c-4db6-e1fd-41db74a32cc3"},"source":["print(sample_msg)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["This is a NLP notebook !. NLP is very useful .Notice: It takes little time to udnerstand NLP techniques\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D8GhnT3zrO1b","colab_type":"code","colab":{},"outputId":"73801561-bc59-4b4b-9225-388b511e27ba"},"source":["#available punctuations\n","string.punctuation"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"Z2_5Pn_LrO1f","colab_type":"code","colab":{}},"source":["#remove punctuations\n","clean_msg=[char for char in sample_msg if char not in string.punctuation]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"futsivq7rO1i","colab_type":"code","colab":{},"outputId":"f41ef0de-fb40-4982-8d36-96f3b3d6254f"},"source":["print(clean_msg)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', ' ', 'N', 'L', 'P', ' ', 'n', 'o', 't', 'e', 'b', 'o', 'o', 'k', ' ', ' ', 'N', 'L', 'P', ' ', 'i', 's', ' ', 'v', 'e', 'r', 'y', ' ', 'u', 's', 'e', 'f', 'u', 'l', ' ', 'N', 'o', 't', 'i', 'c', 'e', ' ', 'I', 't', ' ', 't', 'a', 'k', 'e', 's', ' ', 'l', 'i', 't', 't', 'l', 'e', ' ', 't', 'i', 'm', 'e', ' ', 't', 'o', ' ', 'u', 'd', 'n', 'e', 'r', 's', 't', 'a', 'n', 'd', ' ', 'N', 'L', 'P', ' ', 't', 'e', 'c', 'h', 'n', 'i', 'q', 'u', 'e', 's']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kBi6e9HJrO1n","colab_type":"code","colab":{}},"source":["#use \"\".join to combine the letters\n","clean_msg=\"\".join(clean_msg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oFxke1zArO1r","colab_type":"code","colab":{},"outputId":"9ee9e639-440e-4ac4-cf73-36f52f10c340"},"source":["print(clean_msg)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["This is a NLP notebook  NLP is very useful Notice It takes little time to udnerstand NLP techniques\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0xSJshMprO1v","colab_type":"text"},"source":["# Remove stop words"]},{"cell_type":"code","metadata":{"id":"WzANXPaJrO1v","colab_type":"code","colab":{}},"source":["import nltk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wY_lWwLXrO1z","colab_type":"code","colab":{}},"source":["from nltk.corpus import stopwords"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u0aw558zrO13","colab_type":"code","colab":{},"outputId":"78a90dc6-dbda-42de-ed13-2bf6c151616f"},"source":["stopwords.words('english')[:50]"],"execution_count":null,"outputs":[{"output_type":"error","ename":"LookupError","evalue":"\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/49689/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/49689/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-211e43a58839>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/49689/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"]}]},{"cell_type":"code","metadata":{"id":"P4ZB8VcirO16","colab_type":"code","colab":{}},"source":["This, the, a , an ---"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NKqMsIO6rO19","colab_type":"code","colab":{},"outputId":"32dce668-7dd7-43af-f5ab-ade6f724d604"},"source":["#split the earlier message into words to create a list\n","clean_msg.split()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['This',\n"," 'is',\n"," 'a',\n"," 'NLP',\n"," 'notebook',\n"," 'NLP',\n"," 'is',\n"," 'very',\n"," 'useful',\n"," 'Notice',\n"," 'It',\n"," 'takes',\n"," 'little',\n"," 'time',\n"," 'to',\n"," 'udnerstand',\n"," 'NLP',\n"," 'techniques']"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"-wAdJp0QrO2A","colab_type":"code","colab":{}},"source":["cleaner_msg=[word for word in clean_msg.split() if word.lower() not in stopwords.words('english')]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zo5ggOeCrO2E","colab_type":"code","colab":{},"outputId":"5acac179-f450-454a-9d82-edae239c27d8"},"source":["cleaner_msg"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['NLP',\n"," 'notebook',\n"," 'NLP',\n"," 'useful',\n"," 'Notice',\n"," 'takes',\n"," 'little',\n"," 'time',\n"," 'udnerstand',\n"," 'NLP',\n"," 'techniques']"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"7ydrs9DSrO2L","colab_type":"text"},"source":[" we got rid of the stop words"]},{"cell_type":"code","metadata":{"id":"0njRIm6XrO2M","colab_type":"code","colab":{}},"source":["## create a single function to clean the text \n","def clean_text (text):\n","    \"\"\"\n","    This function cleans the text that is passed as an input\n","    \n","    1. Removes punctuations\n","    2. Removes stopwords\n","    \n","    \"\"\"\n","    \n","    clean_msg=[char for char in text if char not in string.punctuation]\n","    \n","    clean_msg=\"\".join(clean_msg)\n","    \n","    cleaner_msg=[word for word in clean_msg.split() if word.lower() not in stopwords.words('english')]\n","    \n","    return cleaner_msg\n","    \n","    \n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y09v3X32rO2P","colab_type":"code","colab":{}},"source":["This is a machine learnign session on NLP\n","This - not much of info \n","is \n","a\n","on\n","stopwords - get rid of these stopwords  \n","English - 500 words\n","Spanish - 1000\n","Hindi \n","\n","Text becomes more concise and meaningful "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cb678kMQrO2S","colab_type":"code","colab":{},"outputId":"b29760cb-3250-41a3-8ab1-ccbceb2c7c97"},"source":["clean_text('Machine learning is hot cake right now . everyone wants a piece of it yet it takes time and patience !')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Machine',\n"," 'learning',\n"," 'hot',\n"," 'cake',\n"," 'right',\n"," 'everyone',\n"," 'wants',\n"," 'piece',\n"," 'yet',\n"," 'takes',\n"," 'time',\n"," 'patience']"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"ZYM4KUGTrO2U","colab_type":"code","colab":{}},"source":["# Stemming /Lemmatization \n","running - 1 \n","ran - 2 \n","run - 3  \n","root word - run"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5tMZ-p45rO2Z","colab_type":"code","colab":{}},"source":["# Representing text as numerical data\n","sample_msg=['This is introduction to NLP!','it is likely to be useful, to people ','machine learning is the new electricity']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iNBxYPjNrO2e","colab_type":"code","colab":{},"outputId":"173db6d5-8f2d-4c15-a1ca-9add03d6c7b9"},"source":["#Examine the sample text\n","sample_msg"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['This is introduction to NLP!',\n"," 'it is likely to be useful, to people ',\n"," 'machine learning is the new electricity']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"gtd9wNsYrO2h","colab_type":"code","colab":{}},"source":["#import and instantiate Count Vectorizer \n","from sklearn.feature_extraction.text import CountVectorizer\n","vect=CountVectorizer(stop_words=['is'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-PI_fnr9rO2l","colab_type":"code","colab":{}},"source":["1. cleans your data \n","2. Learns unique words in your data \n","3. Creates a vocabulary of distinct words in your data \n","    1. Assigns a number to each word (label encoding)\n","    2. Word has highest frequency - \n","        The -5 - 0\n","        Is -4 - 1\n","        are -2 -2\n","        ..\n","        ..\n","        \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sZGJFf6erO2u","colab_type":"code","colab":{},"outputId":"84decebc-dd88-4714-e4e0-0f19292a7444"},"source":["#Learn the vocab of the sample data\n","vect.fit(sample_msg)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n","                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n","                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n","                ngram_range=(1, 1), preprocessor=None, stop_words=['is'],\n","                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                tokenizer=None, vocabulary=None)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"0reviFrHrO2y","colab_type":"code","colab":{},"outputId":"502b1860-ae90-4c1f-fd6f-bdd0376eb5a5"},"source":["vect.vocabulary_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'this': 11,\n"," 'introduction': 2,\n"," 'to': 12,\n"," 'nlp': 8,\n"," 'it': 3,\n"," 'likely': 5,\n"," 'be': 0,\n"," 'useful': 13,\n"," 'people': 9,\n"," 'machine': 6,\n"," 'learning': 4,\n"," 'the': 10,\n"," 'new': 7,\n"," 'electricity': 1}"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"jQqpabUJrO22","colab_type":"code","colab":{},"outputId":"b5170b91-320d-4115-e37c-0cc7f1eceffe"},"source":["vect.get_feature_names()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['be',\n"," 'electricity',\n"," 'introduction',\n"," 'it',\n"," 'learning',\n"," 'likely',\n"," 'machine',\n"," 'new',\n"," 'nlp',\n"," 'people',\n"," 'the',\n"," 'this',\n"," 'to',\n"," 'useful']"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"J09lF1otrO26","colab_type":"text"},"source":["* Removes punctuation\n","* lowercase\n","* removes duplicate words\n"]},{"cell_type":"code","metadata":{"id":"pv88CQzsrO26","colab_type":"code","colab":{},"outputId":"8e37d5fa-a2b3-4281-91b1-5110dc6695fc"},"source":["#Length of vocabulory\n","len(vect.get_feature_names())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["14"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"FmN2Q_o6rO29","colab_type":"code","colab":{}},"source":["3 rows X14 columns \n","document term matrix "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l9pMs4XMrO3A","colab_type":"code","colab":{},"outputId":"46d2cf15-0600-417d-b1d2-bab771c71468"},"source":["#document term matrix \n","sample_dtm=vect.transform(sample_msg)\n","sample_dtm.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3, 14)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"unitmaCarO3F","colab_type":"code","colab":{},"outputId":"fdf89dd5-f7e4-4306-95a3-fd925188b8fc"},"source":["#length of sample message(Document)\n","len(sample_msg)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"-fLKFHRRrO3H","colab_type":"code","colab":{},"outputId":"744b14d3-701e-464f-e519-e8433d6cc636"},"source":["type(sample_dtm)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["scipy.sparse.csr.csr_matrix"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"anB-_WzLrO3O","colab_type":"code","colab":{},"outputId":"b69d5e2f-67ec-4fa3-a2a5-42e1d0b69d07"},"source":["#View tokens and document term matrix\n","pd.DataFrame(sample_dtm.toarray(),columns=vect.get_feature_names())\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>be</th>\n","      <th>electricity</th>\n","      <th>introduction</th>\n","      <th>it</th>\n","      <th>learning</th>\n","      <th>likely</th>\n","      <th>machine</th>\n","      <th>new</th>\n","      <th>nlp</th>\n","      <th>people</th>\n","      <th>the</th>\n","      <th>this</th>\n","      <th>to</th>\n","      <th>useful</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   be  electricity  introduction  it  learning  likely  machine  new  nlp  \\\n","0   0            0             1   0         0       0        0    0    1   \n","1   1            0             0   1         0       1        0    0    0   \n","2   0            1             0   0         1       0        1    1    0   \n","\n","   people  the  this  to  useful  \n","0       0    0     1   1       0  \n","1       1    0     0   2       1  \n","2       0    1     0   0       0  "]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"I48p2hoPrO3S","colab_type":"code","colab":{}},"source":["# Disadvantages - CV \n","1. Number of columns - Equal to Vocab of CV ( for high dimension - very large matrix)\n","2. Sparse Matrix - Most of the values would be 0 in DTM \n","3. You can't get back to original text from DTM \n","    -Order of words\n","    - Lost stop words\n","4. It doesnt capture the inherent meaning of the word - Embeddings ()\n"],"execution_count":null,"outputs":[]}]}